

# 理论知识

## 如何学习

学习可以分为3部分：获取，思考，行为；我们通过学习某个知识点，对其进行延伸学习、包围学习和主题学习；如何增强记忆呢？1. 快速翻阅，增加隐藏的记忆点，2 回想，越是艰难的东西，越是要尽力去回想，这样才会记得牢固，3 结构化拆解，对其进行深加工。

## 树

二叉查找树：对于树中的每个节点$X$，它的左子树中的所有关键字小于$X$的关键字值，而右子树中所有关键字大于$X$的关键字值，且在二叉树的后序遍历中，根为最后的节点值，可以将之前出现的数据划分为左右子树。

### 二叉查找树 Find

```
def find(x,T):
	if not T:
		return False
	if T.val == x:
		return True
	if x<T.val:
		find(x,T.left)
	else:
		find(x,T.right)
```

注意测试的顺序。首先要对是否为空树进行测试，否则就可能在NULL上浪费时间；其余的测试应该使得最不可能的测试安排在最后进行，以节约判断时间。

### 二叉查找树 delete

如果该node为叶子节点或者只有一个child，则直接删除或者将child节点替代该节点；如果该node有两个child，则将其右子树中的最小节点替换该节点，并删除该最小节点（因为该节点不会有左孩子，所以删除比较容易）。

### AVL中保持平衡的旋转

单旋转

抽象地形容就是：把树形象地看成是灵活柔软的，抓住节点$k_1$，闭上你的双眼，使劲摇动它，在重力作用下，$k_1$ 变成了新的根，而$Y$ 则被撞成$k_2$ 的左子树。 

同理还有以下情形：

如何找到需要旋转的根和子树：根对应的左子树和右子树的深度，差值大于2。

双旋转

对应两次旋转，先找准根，即$k_2$ 所在位置，该节点所对应的子树深度比其他深。

其实是出现以下情况需要用双旋：（出现z字形）出现深度超深的根在原node的左子树的右孩子和右子树的左孩子，否则即可以通过一次旋转到位。

## 摊还分析

计算平均分配的时间，保证之前的动作并减少其访问时间，即最坏时长很少出现。

树的遍历

中序遍历：遍历左孩子，打印该节点值，遍历右孩子

先处理NULL的情况，然后再做其余的工作。

## B-树

阶为M的B-树是一棵具有下列结构特性的树：树的根或者是一片叶子，其儿子数在[2,M]间，除根外，其他的非叶子节点数在[N>=M/2,M]之间。所有的树叶都在相同的深度上。

## hash

哈希表是作为容易插入等操作存在的，而树是作为容易查找而存在的

插入：index = number % table_size，在这中间会存在冲突情况，有多个解决冲突的方案：

$h_i(x)= (Hash(X)+F(i)) mod TabelSize$  且F(0)=0,i=0,1,2,3...

线性探测法：$F(i)=i$ 

平方探测法：$F(i)=i^2$ 

双散列：$F(i)=i*hash_2(X)$ 

再散列：如果表中过于满的话，则生成一个更大的表（新表大小为旧表尺寸两倍后的素数），并将旧表中的元素一一再散列，移到新表中。

### 可扩散列不会

## 优先队列（堆）

完全二叉树：底层上的元素从左到右填入，其余层全部为满的二叉树。

高为h的完全二叉树的所有节点数在$2^h$~$2^{h+1}-1$  ,且其树高为,完全二叉树可以用数组表示，对于数组任意位置上i的元素，其左孩子在数组2i的位置上，而右孩子在（2i+1）的位置上。位置为i的节点的父节点为lower（i/2），堆序指父节点<=子节点的关键字，因此，最小元总在根处被找到。

## 最优化方法

question5.4: 无法复述关于线性规划类的方法，包括牛顿法等等的一系列用矩阵表示的算法

方案5.4：练习并内化该方法

名词：

单纯形法，大M法，分支定界法

### 线性规划

标准形式：目标为最小化，每个变量均大于等于0，每个约束为等式形式。基本变量为正值，自由变量为0.

单纯形法：根据目标函数中的最小系数所对应的变量，将其增加，增加幅度为约束中的最小值，然后将对应的系数变为0，以此类推，直到最终的表格符合以上4点要求。A中有单位字块，右侧非负，底行无负数，且相对于单位子块系数为0.此方法适用于B中为非负元素，即已经基本接近可行解。

大M法：单纯形法的进阶版，增加人工变量，系数为M，修改目标函数，并在最开始将表格修正为标准形式，按照单纯形法计算，得到最终结果。

- [ ] 对偶单纯形法：右列元素不为非负型，而其他特点均已具备，选取负值且对应位置的比最接近于0，（也可以认为是绝对值小的），单纯形法和对偶单纯形法可以交替使用。
- [ ] 目标函数中$c_i$的灵敏度分析，其实是各个变量间的重要性，因为标准的是求max；而$b_i$变化则是作为拥有资源的变化，（如果没有剩余变量时），会使得整体的目标值变大，但如果它具有剩余变量，即该资源并没有使用完而是取决于其他已经用完的变量。

![1526049462735](C:\Users\zhouying\AppData\Local\Temp\1526049462735.png)

### 无约束线性规划

当我们要求解一个问题时，通常在找出求解方法之前，先研究该问题的解所满足的条件或者应该具备的性质，以便为寻找求解方法而拓广思路。

二分法：迭代产生解序列

0.618法：每次成比例保留区间：

根据成比例保留原则，$\frac{x_1-a}{b-a}=\frac{x_2-a}{x_1-a}$ ,设$\frac{x_1-a}{b-a}=t$ 则等式最终可以化为$t^2=1-t$ 

注意：第三次试验点在$x_2$的左边，否则等比例舍弃将被违反。即$\frac{1-t}{t}=t$ 

### 多目标规划

标准形式：min多个目标函数，受到的约束同之前的相同。

方案1：加权，可以按照先验知识加权；也可以根据各个函数的独立最小值，计算其相对权重；这个按照时间相同来进行

方案2：按照时间顺序，对重要的目标函数先进行计算，对其可行域内的进行以下目标函数计算，根据每一个目标函数求得的域中，把硬性条件直接替换，然后再对下一个目标函数进行计算。Q：影响计算时间

注：之前在算法课上，老师求了一个最大的和，因此也可以按照重要程度分解，看做是多个max目标，之前满足条件的变量不做改变就是为了保持最开始的条件是满足的。

### 非凸优化

1 Lipschitz continuous

![preview](https://pic1.zhimg.com/v2-2fccac782e27798e1a986976fb5e617b_r.jpg)

这个是指，在f函数上的点，用某已知点的函数值来拟合已知点，则两者间的误差不会超过两个自变量的线性值的某个倍数范围。

![img](https://pic4.zhimg.com/80/v2-c1a6c6b5358f98e1b4407e514587b1c6_hd.jpg)

如果函数![f](https://www.zhihu.com/equation?tex=f)的**导函数**![f^{\prime}](https://www.zhihu.com/equation?tex=f%5E%7B%5Cprime%7D)是Lipschitz continuous，那么我们说函数![f](https://www.zhihu.com/equation?tex=f)符合Lipschitz  continuous gradient ；如果函数![f](https://www.zhihu.com/equation?tex=f)的**Hessien**![f^{\prime\prime}](https://www.zhihu.com/equation?tex=f%5E%7B%5Cprime%5Cprime%7D)是Lipschitz continuous，那么我们说函数![f](https://www.zhihu.com/equation?tex=f)符合Lipschitz  continuous Hessian.

![img](https://pic4.zhimg.com/80/v2-eebeeb9cdf0daac6bf2b89a099997372_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-44a9dc6d8d8071fca6a9fc31fd837e8e_hd.jpg)

![img](https://pic1.zhimg.com/80/v2-71da3c85a46f3dd5ce1182d427318ec8_hd.jpg)

![img](https://pic4.zhimg.com/80/v2-dcfd76344eda1f34006791b196a95fbe_hd.jpg)

前者是Lipschitz continuous gradient，后者是lipschitz continuous hessian

限制函数的变化速度，不能太快。

## 机器学习

定义：特征，样本，训练集合等

![1526562772608](C:\Users\zhouying\AppData\Local\Temp\1526562772608.png)

![1526562824683](C:\Users\zhouying\AppData\Local\Temp\1526562824683.png)

理解：原来ground truth翻译为真相，而这里的 学习算法在给定数据和参数空间的实例化是个很有意思的说明，但细想确实如此。

学习算法有其特定假设，这些假设有些是为了逼近ground truth，有些是为了简化计算，而数据和样本，则提供了算法实例化的信息，学习出相应的模型。

归纳学习：从实例中学习。归纳与演绎是科学推理的两大基本手段。

由于假设空间过大而样本数量不足，导致多个假设均符合训练数据，此时，归纳偏好产生作用，在归纳偏好下，从中选择某个假设。

![1526563464891](C:\Users\zhouying\AppData\Local\Temp\1526563464891.png)

![1526563976281](C:\Users\zhouying\AppData\Local\Temp\1526563976281.png)

![1526564867279](C:\Users\zhouying\AppData\Local\Temp\1526564867279.png)

![1526565581249](C:\Users\zhouying\AppData\Local\Temp\1526565581249.png)

![1526565925199](C:\Users\zhouying\AppData\Local\Temp\1526565925199.png)

查准率是指给出的答案有多少正确的，而查全率是指全部正确答案中找到了多少。

![1526566085573](C:\Users\zhouying\AppData\Local\Temp\1526566085573.png)

![1526566354282](C:\Users\zhouying\AppData\Local\Temp\1526566354282.png)

从这个过程中可以看到，如果算法性能较好，则roc曲线会尽早趋近于（0,1)值。

![1526566562468](C:\Users\zhouying\AppData\Local\Temp\1526566562468.png)

这个公式的设计很巧妙，从计算过程来看，应该是锯齿下的长乘以宽，但是由于无法确定矩形的高度，因此采用了固定的算法，而除以2的设计。

![1526567304690](C:\Users\zhouying\AppData\Local\Temp\1526567304690.png)

![1526568575701](C:\Users\zhouying\AppData\Local\Temp\1526568575701.png)

![1526608262098](C:\Users\zhouying\AppData\Local\Temp\1526608262098.png)

逻辑回归：

$ln\frac{y}{1-y}=w^Tx+b$  将$y$视为样本$x$作为正例的可能性，则$1-y$ 是其反例的可能性，即用线性函数对其正例可能性与反例可能性的比值作为回归目标。

![1526624165193](C:\Users\zhouying\AppData\Local\Temp\1526624165193.png)

在阈值决策时，可以认为$y$是样本$x$作为正例的可能性，则当y大于0.5时，该样本会在最终被预测为正例

因此，这个决策行为可以看做是当y大于先验的时候，即会预测为正例，则有一下结论：$y>\frac{m_+}{m_++m_-}$，而$1-y>\frac{m_-}{m_++m_-}$

因此有$\frac{y}{1-y}>\frac{m_+}{m_-}$ 即只要分类器的预测几率大于观测几率，即可判定为正例

##### 神经网络

![1526632655933](C:\Users\zhouying\AppData\Local\Temp\1526632655933.png)

![1526632686982](C:\Users\zhouying\AppData\Local\Temp\1526632686982.png)

![1526632713742](C:\Users\zhouying\AppData\Local\Temp\1526632713742.png)

## 神经网络

### loss函数设计

成熟的包中，优化函数是以minize某个loss函数为目标的，因此，loss函数应当是以0为最小值点，且原来是大于0的函数的，最常见的是MSE和交叉熵等。

#### 多任务学习/多个loss函数融合

在多任务学习中，不是单纯的设计好不同任务的loss函数然后加权即可，tasks之间彼此的相容性对结果也会有一些影响。当两个任务矛盾的时候， 往往结果会比单任务还要差不少。Multi-task learning 还需要解决的是Gradient domination的问题。 这个问题产生的原因是不同任务的loss的梯度相差过大， 导致梯度小的loss在训练过程中被梯度大的loss所带走。 如果一开始就给不同的Loss进行加权， 让它们有相近的梯度， 是不是就能训练的好呢？   结果往往不是这样的。 **不同的loss， 他们的梯度在训练过程中变化情况也是不一样的；而且不同的loss, 在梯度值相同的时候， 它们在task上的表现也是不同的。**在训练开始的时候，虽然balance了， 但是随着训练过程的进行， 中间又发生gradient domination了。  所以要想解决这个问题， 还是要合适地对不同loss做合适的均衡。

实践中应该要如何调整呢？ 其实很简单：

假设我们有两个task, 用A和B表示。   假设网络设计足够好， 容量足够大， 而且两个任务本身具有相关性，能够训得足够好。

如果A和B单独训练， 他们在收敛的时候的梯度大小分别记为 Grad_a*,* Grad_b*， 那么我们只需要在两个任务一起训练的时候， 分别用各自梯度的倒数（*1/Grad_a, 1/Grad_b）对两个任务做平衡， 然后统一乘一个scalar就可以了。(根据单任务的收敛时候的loss梯度去确定multi-task训练中不同任务的权重。)

因为loss的梯度在训练中通常会变小（这里用通常是因为一般mean square error等loss是这样， 其他有的Loss并不是。）， 如果我们确定这个网络在multi-task训练的时候能够达到原来的效果， 我们就只需要把平衡点设在两个任务都足够好的时候。 这样网络在训练过程中， 就自然能够达到那个平衡点， 即使一开始的时候会有gradient domination出现。