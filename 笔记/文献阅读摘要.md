---
typora-root-url: image
typora-copy-images-to: image
---

### 最大化F1值计算过程

[1]: https://blog.argcv.com/articles/1036.c	"分类问题中的各种evaluation"

问题来源：传统的分类器只能利用精度作为全局优化函数，但在不平衡分类问题中，全局精度会导致分类器倾向于将样本区分为多数类，从而忽视少数类样本的识别。

idea：将分类器的少数类的分类performance直接作为优化函数

神经网络的自定义目标函数：

传统的目标函数：

$J(W;x^{(i)},y^{(i)})=\frac{1}{2}{||h(x^{(i)})-y^{(i)}||}^2$ 

考虑加入$F1$ 的目标函数，并不是平方误差越小越好，而是尽可能准的辨认出正类样本。

 $y =sgn(h(x))=\begin{cases}0 & ,h(x)<0.5\\1 & ,h(x) \geq 0.5\end{cases} $

$loss = \frac{2*h(x)*y}{{h(x)}^2+y^2}$

内积：$a.b=\sum{a_ib_i}$

### 生成式对抗网络研究进展 

![](F:\OneDrive\笔记\image\vae缺点.png)

![1525677322963](/1525677322963.png)

![1525677582680](/1525677582680.png)

![1525680669967](/1525680669967.png)

不太明白有效的推断机制是指什么？

流型学习：http://blog.pluskid.org/?p=533 低维流型和高维流型可以认为是空间等，即$R^n$与$R^m$之间的关系等，

![1525683912036](/1525683912036.png)

标签平滑与单边标签平滑

在神经网络中使用标签平滑是为了在$log$目标函数中，为了避免在bp过程中出现梯度过大问题，才使用了标签平滑，即用0.1代替0，而用0.9代替1，但在GAN中，如果直接采用标签平滑，则原来的最优分布$D^*=\frac{P_{data}}{P_{data}+P_{model}}$变为$D^*=\frac{\alpha P_{data}+\beta P_{model}}{P_{data}+P_{model}}$，因为$P_{model}$出现在分子中，当$P_{data}$接近于0而$P_{model}$较大时，model所生成的样本便不会趋向于真实样本。

这个故事的解释是：当空间中某一点的真实性较差，但在模型中计算的概率较大时，模型不会修改自己的错误，而是会一直保持，即生成不真实的样本点。

因此，单边平滑是指仅变化$\alpha$而$\beta$设为0，因此仅对真实样本的标签进行平滑，而生成样本的标签始终设为0.

即，修改的是$D_{loss}$ 而由此导致的贝叶斯最优分类器是原来的$\alpha$倍

批处理方法 batch normalization

![1525761096284](/1525761096284.png)

![1525761698800](/1525761698800.png)

LSGAN

http://blog.sciencenet.cn/home.php?mod=space&uid=3291369&do=blog&id=1071536 LSGAN 限制真实分布和假设分布的分布形式，符合lipschitz continuous 条件，即分布变化不会过于剧烈，在此条件下，只是对G的生成样本进行评分，而并不是完全使用D来判断，同时目标不是直接定义，而是利用神经网络作为拟合。

imformed sampling

首先给出不平衡问题产生的现实原因：由于类别间的关注度不同。

并划分出不平衡问题的几个大类：

首先是问题的定义，何为不平衡，这篇文章还是以不平衡率指标来衡量，随着不平衡率增加，不平衡问题显得更加严重。

![1526395083951](/1526395083951.png)

同时，这里还有一个假设，即训练数据集可以反映整个数据集的分布，否则难以起作用。

